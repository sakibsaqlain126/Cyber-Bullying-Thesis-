{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "0b520785",
   "metadata": {
    "_cell_guid": "696879c5-6c51-4a8c-a62c-53a317b476fd",
    "_uuid": "e823abfb-e2a8-47d8-8719-480efa49f815",
    "collapsed": false,
    "execution": {
     "iopub.execute_input": "2025-12-04T18:01:08.222648Z",
     "iopub.status.busy": "2025-12-04T18:01:08.222358Z",
     "iopub.status.idle": "2025-12-04T18:19:37.299452Z",
     "shell.execute_reply": "2025-12-04T18:19:37.298733Z"
    },
    "jupyter": {
     "outputs_hidden": false
    },
    "papermill": {
     "duration": 1109.083518,
     "end_time": "2025-12-04T18:19:37.302133",
     "exception": false,
     "start_time": "2025-12-04T18:01:08.218615",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "üì• Loading dataset...\n",
      "Loaded dataset shape: (47692, 2)\n",
      "Columns: ['tweet_text', 'cyberbullying_type']\n",
      "üßπ Preprocessing texts (advanced)...\n",
      "After cleaning: (47252, 3)\n",
      "üî¢ Class mapping: {'age': 0, 'ethnicity': 1, 'gender': 2, 'not_cyberbullying': 3, 'other_cyberbullying': 4, 'religion': 5}\n",
      "üî§ Fitting TF-IDF...\n",
      "üîÄ Train-test split...\n",
      "Train size: 37801, Test size: 9451\n",
      "üå≤ Training Random Forest (no GridSearch)...\n",
      "‚úÖ Training completed in 57.66 s\n",
      "üìä Evaluating...\n",
      "‚è≥ Computing learning curve (this may take time)...\n",
      "\n",
      "==== METRICS SUMMARY ====\n",
      "accuracy: 0.8106020526928367\n",
      "precision_macro: 0.8095936352438643\n",
      "recall_macro: 0.8076337092831797\n",
      "f1_macro: 0.8081756353679146\n",
      "precision_weighted: 0.8128873442490423\n",
      "recall_weighted: 0.8106020526928367\n",
      "f1_weighted: 0.8113175172261858\n",
      "mcc: 0.772968715468762\n",
      "cohen_kappa: 0.7727324417796563\n",
      "balanced_accuracy: 0.8076337092831797\n",
      "log_loss: 0.553700817262767\n",
      "roc_auc_macro: 0.9566576399025016\n",
      "average_precision_macro: 0.8233209156377993\n",
      "train_time_sec: 57.66289949417114\n",
      "model_size_mb: 348.4649438858032\n",
      "n_classes: 6\n",
      "n_features: 5000\n",
      "train_samples: 37801\n",
      "test_samples: 9451\n",
      "\n",
      "Files written to: /kaggle/working/random_forest_outputs\n"
     ]
    }
   ],
   "source": [
    "# Final corrected pipeline: Advanced preprocessing + Random Forest + conference-ready metrics\n",
    "# Saves outputs to /kaggle/working/random_forest_outputs\n",
    "\n",
    "import time\n",
    "import json\n",
    "import os\n",
    "import re\n",
    "import string\n",
    "import joblib\n",
    "import nltk\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from pathlib import Path\n",
    "\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.stem import PorterStemmer\n",
    "from sklearn.model_selection import train_test_split, learning_curve, cross_val_score\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.preprocessing import LabelEncoder, label_binarize\n",
    "from sklearn.metrics import (\n",
    "    accuracy_score, precision_score, recall_score, f1_score,\n",
    "    classification_report, confusion_matrix, roc_curve, roc_auc_score,\n",
    "    precision_recall_curve, average_precision_score, log_loss,\n",
    "    matthews_corrcoef, cohen_kappa_score, balanced_accuracy_score\n",
    ")\n",
    "\n",
    "# -------------------------\n",
    "# Config\n",
    "# -------------------------\n",
    "INPUT_CSV = \"/kaggle/input/cyberbullying-classification/cyberbullying_tweets.csv\"\n",
    "OUTPUT_DIR = \"/kaggle/working/random_forest_outputs\"\n",
    "os.makedirs(OUTPUT_DIR, exist_ok=True)\n",
    "sns.set_palette(\"husl\")\n",
    "plt.style.use(\"seaborn-v0_8\")\n",
    "\n",
    "# -------------------------\n",
    "# NLTK setup\n",
    "# -------------------------\n",
    "nltk.download(\"stopwords\", quiet=True)\n",
    "stop_words = set(stopwords.words(\"english\"))\n",
    "ps = PorterStemmer()\n",
    "\n",
    "# -------------------------\n",
    "# Load dataset (assumes columns: tweet_text, cyberbullying_type)\n",
    "# -------------------------\n",
    "print(\"üì• Loading dataset...\")\n",
    "df = pd.read_csv(INPUT_CSV)\n",
    "print(f\"Loaded dataset shape: {df.shape}\")\n",
    "print(\"Columns:\", list(df.columns))\n",
    "\n",
    "# Validate expected columns\n",
    "TEXT_COL = \"tweet_text\"\n",
    "TARGET_COL = \"cyberbullying_type\"\n",
    "if TEXT_COL not in df.columns or TARGET_COL not in df.columns:\n",
    "    raise ValueError(f\"Expected columns '{TEXT_COL}' and '{TARGET_COL}' in CSV. Found: {list(df.columns)}\")\n",
    "\n",
    "# -------------------------\n",
    "# Advanced preprocessing\n",
    "# -------------------------\n",
    "def advanced_preprocess(text):\n",
    "    if pd.isna(text):\n",
    "        return \"\"\n",
    "    s = str(text).lower()\n",
    "    # remove urls, mentions, hashtags\n",
    "    s = re.sub(r\"http\\S+|www\\.\\S+|@\\w+|#\\w+\", \" \", s)\n",
    "    # remove punctuation\n",
    "    s = s.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "    # remove digits\n",
    "    s = re.sub(r\"\\d+\", \" \", s)\n",
    "    # normalize whitespace\n",
    "    s = re.sub(r\"\\s+\", \" \", s).strip()\n",
    "    # tokenize\n",
    "    tokens = s.split()\n",
    "    # remove stopwords and single-char tokens, apply stemming\n",
    "    tokens = [ps.stem(tok) for tok in tokens if tok not in stop_words and len(tok) > 1]\n",
    "    return \" \".join(tokens)\n",
    "\n",
    "print(\"üßπ Preprocessing texts (advanced)...\")\n",
    "df[\"clean_text\"] = df[TEXT_COL].apply(advanced_preprocess)\n",
    "# drop empty rows after cleaning\n",
    "df = df[df[\"clean_text\"].str.len() > 0].reset_index(drop=True)\n",
    "print(f\"After cleaning: {df.shape}\")\n",
    "\n",
    "# -------------------------\n",
    "# Encode labels\n",
    "# -------------------------\n",
    "le = LabelEncoder()\n",
    "df[\"label_enc\"] = le.fit_transform(df[TARGET_COL])\n",
    "joblib.dump(le, f\"{OUTPUT_DIR}/label_encoder.pkl\")\n",
    "print(\"üî¢ Class mapping:\", dict(zip(le.classes_, le.transform(le.classes_))))\n",
    "\n",
    "# Save dataset info (pre/post)\n",
    "dataset_info = {\n",
    "    \"original_rows\": int(pd.read_csv(INPUT_CSV).shape[0]),\n",
    "    \"rows_after_cleaning\": int(df.shape[0]),\n",
    "    \"text_column\": TEXT_COL,\n",
    "    \"target_column\": TARGET_COL,\n",
    "    \"classes\": list(le.classes_),\n",
    "    \"class_counts\": {str(k): int(v) for k, v in df[\"label_enc\"].value_counts().to_dict().items()}\n",
    "}\n",
    "with open(f\"{OUTPUT_DIR}/dataset_info.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(dataset_info, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# -------------------------\n",
    "# TF-IDF (fit on cleaned text)\n",
    "# -------------------------\n",
    "print(\"üî§ Fitting TF-IDF...\")\n",
    "tfidf = TfidfVectorizer(max_features=5000, ngram_range=(1,2), min_df=2, max_df=0.95)\n",
    "X = tfidf.fit_transform(df[\"clean_text\"])\n",
    "y = df[\"label_enc\"].values\n",
    "joblib.dump(tfidf, f\"{OUTPUT_DIR}/tfidf_vectorizer.pkl\")\n",
    "\n",
    "# -------------------------\n",
    "# Train-test split\n",
    "# -------------------------\n",
    "print(\"üîÄ Train-test split...\")\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20, random_state=42, stratify=y)\n",
    "print(f\"Train size: {X_train.shape[0]}, Test size: {X_test.shape[0]}\")\n",
    "\n",
    "# -------------------------\n",
    "# Train Random Forest\n",
    "# -------------------------\n",
    "print(\"üå≤ Training Random Forest (no GridSearch)...\")\n",
    "rf = RandomForestClassifier(n_estimators=200, random_state=42, n_jobs=-1, class_weight=\"balanced\")\n",
    "t0 = time.time()\n",
    "rf.fit(X_train, y_train)\n",
    "t1 = time.time()\n",
    "train_time = t1 - t0\n",
    "joblib.dump(rf, f\"{OUTPUT_DIR}/random_forest_model.pkl\")\n",
    "print(f\"‚úÖ Training completed in {train_time:.2f} s\")\n",
    "\n",
    "# -------------------------\n",
    "# Basic evaluation\n",
    "# -------------------------\n",
    "print(\"üìä Evaluating...\")\n",
    "y_pred = rf.predict(X_test)\n",
    "try:\n",
    "    y_proba = rf.predict_proba(X_test)\n",
    "except Exception:\n",
    "    y_proba = None\n",
    "\n",
    "acc = accuracy_score(y_test, y_pred)\n",
    "prec_macro = precision_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "rec_macro = recall_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "f1_macro = f1_score(y_test, y_pred, average=\"macro\", zero_division=0)\n",
    "prec_weight = precision_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "rec_weight = recall_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "f1_weight = f1_score(y_test, y_pred, average=\"weighted\", zero_division=0)\n",
    "mcc = matthews_corrcoef(y_test, y_pred)\n",
    "kappa = cohen_kappa_score(y_test, y_pred)\n",
    "bal_acc = balanced_accuracy_score(y_test, y_pred)\n",
    "try:\n",
    "    ll = float(log_loss(y_test, y_proba)) if y_proba is not None else None\n",
    "except Exception:\n",
    "    ll = None\n",
    "\n",
    "# classification report (txt + csv)\n",
    "clf_rep_txt = classification_report(y_test, y_pred, target_names=le.classes_, zero_division=0)\n",
    "with open(f\"{OUTPUT_DIR}/classification_report_rf.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(clf_rep_txt)\n",
    "pd.DataFrame(classification_report(y_test, y_pred, output_dict=True)).transpose().to_csv(f\"{OUTPUT_DIR}/classification_report_rf.csv\")\n",
    "\n",
    "# confusion matrix (csv + png)\n",
    "cm = confusion_matrix(y_test, y_pred)\n",
    "pd.DataFrame(cm, index=le.classes_, columns=le.classes_).to_csv(f\"{OUTPUT_DIR}/confusion_matrix_rf.csv\")\n",
    "plt.figure(figsize=(8,6))\n",
    "sns.heatmap(cm, annot=True, fmt=\"d\", xticklabels=le.classes_, yticklabels=le.classes_, cmap=\"Blues\")\n",
    "plt.xlabel(\"Predicted\")\n",
    "plt.ylabel(\"True\")\n",
    "plt.title(\"Confusion Matrix - Random Forest\")\n",
    "plt.tight_layout()\n",
    "plt.savefig(f\"{OUTPUT_DIR}/confusion_matrix_rf.png\", dpi=300)\n",
    "plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# ROC & AUC (per-class) and PR & AUPRC\n",
    "# -------------------------\n",
    "n_classes = len(le.classes_)\n",
    "y_test_bin = label_binarize(y_test, classes=range(n_classes))\n",
    "roc_auc_per_class = {}\n",
    "pr_auc_per_class = {}\n",
    "roc_auc_macro = None\n",
    "avg_precision_macro = None\n",
    "\n",
    "if y_proba is not None:\n",
    "    plt.figure(figsize=(7,6))\n",
    "    aucs = []\n",
    "    for i in range(n_classes):\n",
    "        try:\n",
    "            fpr, tpr, _ = roc_curve(y_test_bin[:, i], y_proba[:, i])\n",
    "            auc_i = roc_auc_score(y_test_bin[:, i], y_proba[:, i])\n",
    "            aucs.append(auc_i)\n",
    "            plt.plot(fpr, tpr, label=f\"{le.classes_[i]} (AUC={auc_i:.3f})\")\n",
    "            roc_auc_per_class[le.classes_[i]] = float(auc_i)\n",
    "        except Exception:\n",
    "            continue\n",
    "    if len(aucs) > 0:\n",
    "        roc_auc_macro = float(np.mean(aucs))\n",
    "    plt.plot([0,1],[0,1],\"k--\")\n",
    "    plt.xlabel(\"False Positive Rate\")\n",
    "    plt.ylabel(\"True Positive Rate\")\n",
    "    plt.title(\"ROC Curves - Random Forest (per class)\")\n",
    "    plt.legend(loc=\"lower right\", fontsize=\"small\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/roc_curve_rf.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "    # Precision-Recall\n",
    "    plt.figure(figsize=(7,6))\n",
    "    aps = []\n",
    "    for i in range(n_classes):\n",
    "        try:\n",
    "            precision, recall, _ = precision_recall_curve(y_test_bin[:, i], y_proba[:, i])\n",
    "            ap_i = average_precision_score(y_test_bin[:, i], y_proba[:, i])\n",
    "            aps.append(ap_i)\n",
    "            plt.plot(recall, precision, label=f\"{le.classes_[i]} (AP={ap_i:.3f})\")\n",
    "            pr_auc_per_class[le.classes_[i]] = float(ap_i)\n",
    "        except Exception:\n",
    "            continue\n",
    "    if len(aps) > 0:\n",
    "        avg_precision_macro = float(np.mean(aps))\n",
    "    plt.xlabel(\"Recall\")\n",
    "    plt.ylabel(\"Precision\")\n",
    "    plt.title(\"Precision-Recall Curves - Random Forest (per class)\")\n",
    "    plt.legend(loc=\"lower left\", fontsize=\"small\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/pr_curve_rf.png\", dpi=300)\n",
    "    plt.close()\n",
    "\n",
    "# -------------------------\n",
    "# Feature importances (top-k)\n",
    "# -------------------------\n",
    "try:\n",
    "    feat_imp = rf.feature_importances_\n",
    "    feat_names = tfidf.get_feature_names_out()\n",
    "    fi_df = pd.DataFrame({\"feature\": feat_names, \"importance\": feat_imp})\n",
    "    fi_df = fi_df.sort_values(\"importance\", ascending=False)\n",
    "    fi_df.to_csv(f\"{OUTPUT_DIR}/feature_importances_rf.csv\", index=False)\n",
    "    top_n = min(30, len(fi_df))\n",
    "    plt.figure(figsize=(8, max(4, top_n//2)))\n",
    "    plt.barh(fi_df[\"feature\"].values[:top_n][::-1], fi_df[\"importance\"].values[:top_n][::-1])\n",
    "    plt.title(\"Top feature importances - Random Forest\")\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/feature_importances_rf.png\", dpi=300)\n",
    "    plt.close()\n",
    "except Exception as e:\n",
    "    print(\"Feature importances skipped or error:\", e)\n",
    "\n",
    "# -------------------------\n",
    "# Learning curve (guarded)\n",
    "# -------------------------\n",
    "try:\n",
    "    print(\"‚è≥ Computing learning curve (this may take time)...\")\n",
    "    train_sizes, train_scores, test_scores = learning_curve(rf, X, y, cv=5, n_jobs=-1,\n",
    "                                                            train_sizes=np.linspace(0.1,1.0,5), scoring=\"accuracy\")\n",
    "    train_mean = np.mean(train_scores, axis=1)\n",
    "    test_mean = np.mean(test_scores, axis=1)\n",
    "    plt.figure(figsize=(8,6))\n",
    "    plt.plot(train_sizes, train_mean, 'o-', label='Train')\n",
    "    plt.plot(train_sizes, test_mean, 'o-', label='CV')\n",
    "    plt.xlabel(\"Training examples\")\n",
    "    plt.ylabel(\"Accuracy\")\n",
    "    plt.title(\"Learning Curve - Random Forest\")\n",
    "    plt.legend(loc='best')\n",
    "    plt.grid(alpha=0.3)\n",
    "    plt.tight_layout()\n",
    "    plt.savefig(f\"{OUTPUT_DIR}/learning_curve_rf.png\", dpi=300)\n",
    "    plt.close()\n",
    "except Exception as e:\n",
    "    print(\"Learning curve computation skipped or failed:\", e)\n",
    "\n",
    "# -------------------------\n",
    "# Cross-validation summary\n",
    "# -------------------------\n",
    "try:\n",
    "    cv_scores = cross_val_score(rf, X, y, cv=5, scoring=\"accuracy\", n_jobs=-1)\n",
    "    cv_summary = {\"cv_scores\": [float(s) for s in cv_scores.tolist()], \"cv_mean\": float(np.mean(cv_scores)), \"cv_std\": float(np.std(cv_scores))}\n",
    "    with open(f\"{OUTPUT_DIR}/cv_summary_rf.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "        json.dump(cv_summary, f, indent=4, ensure_ascii=False)\n",
    "except Exception as e:\n",
    "    print(\"CV summary skipped or failed:\", e)\n",
    "\n",
    "# -------------------------\n",
    "# Inference timing: 3 samples and full test set\n",
    "# -------------------------\n",
    "sample_texts = [\n",
    "    \"I love this community, it's very helpful\",\n",
    "    \"You're an idiot, go away\",\n",
    "    \"Women don't belong in tech\"\n",
    "]\n",
    "sample_clean = [advanced_preprocess(t) for t in sample_texts]\n",
    "sample_vec = tfidf.transform(sample_clean)\n",
    "\n",
    "# predict batch timing\n",
    "t0 = time.perf_counter()\n",
    "sample_preds = rf.predict(sample_vec)\n",
    "t1 = time.perf_counter()\n",
    "total_batch_pred = t1 - t0\n",
    "avg_batch_pred = total_batch_pred / len(sample_texts)\n",
    "\n",
    "# predict loop timing\n",
    "t0 = time.perf_counter()\n",
    "for i in range(len(sample_texts)):\n",
    "    _ = rf.predict(sample_vec[i])\n",
    "t1 = time.perf_counter()\n",
    "avg_loop_pred = (t1 - t0) / len(sample_texts)\n",
    "\n",
    "# full test timing\n",
    "t0 = time.perf_counter()\n",
    "_ = rf.predict(X_test)\n",
    "t1 = time.perf_counter()\n",
    "total_test_pred = t1 - t0\n",
    "avg_test_pred = total_test_pred / X_test.shape[0]\n",
    "\n",
    "# predict_proba timings if available\n",
    "total_proba_batch = None\n",
    "avg_proba_batch = None\n",
    "total_proba_test = None\n",
    "avg_proba_test = None\n",
    "if hasattr(rf, \"predict_proba\"):\n",
    "    t0 = time.perf_counter()\n",
    "    _ = rf.predict_proba(sample_vec)\n",
    "    t1 = time.perf_counter()\n",
    "    total_proba_batch = t1 - t0\n",
    "    avg_proba_batch = total_proba_batch / len(sample_texts)\n",
    "    t0 = time.perf_counter()\n",
    "    _ = rf.predict_proba(X_test)\n",
    "    t1 = time.perf_counter()\n",
    "    total_proba_test = t1 - t0\n",
    "    avg_proba_test = total_proba_test / X_test.shape[0]\n",
    "\n",
    "inference_info = {\n",
    "    \"sample_texts\": sample_texts,\n",
    "    \"num_samples\": len(sample_texts),\n",
    "    \"total_batch_predict_sec\": float(total_batch_pred),\n",
    "    \"avg_batch_predict_sec_per_sample\": float(avg_batch_pred),\n",
    "    \"avg_loop_predict_sec_per_sample\": float(avg_loop_pred),\n",
    "    \"total_test_predict_sec\": float(total_test_pred),\n",
    "    \"avg_test_predict_sec_per_sample\": float(avg_test_pred),\n",
    "    \"total_proba_batch_sec\": float(total_proba_batch) if total_proba_batch is not None else None,\n",
    "    \"avg_proba_batch_sec_per_sample\": float(avg_proba_batch) if avg_proba_batch is not None else None,\n",
    "    \"total_proba_test_sec\": float(total_proba_test) if total_proba_test is not None else None,\n",
    "    \"avg_proba_test_sec_per_sample\": float(avg_proba_test) if avg_proba_test is not None else None\n",
    "}\n",
    "with open(f\"{OUTPUT_DIR}/inference_time_rf.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(inference_info, f, indent=4, ensure_ascii=False)\n",
    "with open(f\"{OUTPUT_DIR}/inference_time_rf.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    for k, v in inference_info.items():\n",
    "        f.write(f\"{k}: {v}\\n\")\n",
    "\n",
    "# -------------------------\n",
    "# Model size\n",
    "# -------------------------\n",
    "model_path = Path(f\"{OUTPUT_DIR}/random_forest_model.pkl\")\n",
    "model_size_mb = model_path.stat().st_size / (1024*1024)\n",
    "with open(f\"{OUTPUT_DIR}/model_size_rf.txt\", \"w\", encoding=\"utf-8\") as f:\n",
    "    f.write(f\"model_size_mb: {model_size_mb:.4f}\\n\")\n",
    "\n",
    "# -------------------------\n",
    "# Metrics summary JSON\n",
    "# -------------------------\n",
    "metrics_summary = {\n",
    "    \"accuracy\": float(acc),\n",
    "    \"precision_macro\": float(prec_macro),\n",
    "    \"recall_macro\": float(rec_macro),\n",
    "    \"f1_macro\": float(f1_macro),\n",
    "    \"precision_weighted\": float(prec_weight),\n",
    "    \"recall_weighted\": float(rec_weight),\n",
    "    \"f1_weighted\": float(f1_weight),\n",
    "    \"mcc\": float(mcc),\n",
    "    \"cohen_kappa\": float(kappa),\n",
    "    \"balanced_accuracy\": float(bal_acc),\n",
    "    \"log_loss\": float(ll) if ll is not None else None,\n",
    "    \"roc_auc_macro\": float(roc_auc_macro) if roc_auc_macro is not None else None,\n",
    "    \"average_precision_macro\": float(avg_precision_macro) if avg_precision_macro is not None else None,\n",
    "    \"train_time_sec\": float(train_time),\n",
    "    \"model_size_mb\": float(model_size_mb),\n",
    "    \"n_classes\": int(n_classes),\n",
    "    \"n_features\": int(X.shape[1]),\n",
    "    \"train_samples\": int(X_train.shape[0]),\n",
    "    \"test_samples\": int(X_test.shape[0])\n",
    "}\n",
    "with open(f\"{OUTPUT_DIR}/metrics_summary_rf.json\", \"w\", encoding=\"utf-8\") as f:\n",
    "    json.dump(metrics_summary, f, indent=4, ensure_ascii=False)\n",
    "\n",
    "# -------------------------\n",
    "# Sample predictions saved\n",
    "# -------------------------\n",
    "sample_pred_labels = le.inverse_transform(sample_preds)\n",
    "sample_df = pd.DataFrame({\"text\": sample_texts, \"cleaned\": sample_clean, \"predicted\": sample_pred_labels})\n",
    "sample_df.to_csv(f\"{OUTPUT_DIR}/sample_predictions_rf.csv\", index=False)\n",
    "\n",
    "# -------------------------\n",
    "# Final summary print\n",
    "# -------------------------\n",
    "print(\"\\n==== METRICS SUMMARY ====\")\n",
    "for k, v in metrics_summary.items():\n",
    "    print(f\"{k}: {v}\")\n",
    "print(\"\\nFiles written to:\", OUTPUT_DIR)\n"
   ]
  }
 ],
 "metadata": {
  "kaggle": {
   "accelerator": "gpu",
   "dataSources": [
    {
     "datasetId": 1869236,
     "sourceId": 3053020,
     "sourceType": "datasetVersion"
    }
   ],
   "dockerImageVersionId": 31041,
   "isGpuEnabled": true,
   "isInternetEnabled": true,
   "language": "python",
   "sourceType": "notebook"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.11"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 1115.829475,
   "end_time": "2025-12-04T18:19:39.921523",
   "environment_variables": {},
   "exception": null,
   "input_path": "__notebook__.ipynb",
   "output_path": "__notebook__.ipynb",
   "parameters": {},
   "start_time": "2025-12-04T18:01:04.092048",
   "version": "2.6.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
